{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'layer1/conv2d/kernel:0' shape=(3, 3, 3, 32) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'layer1/conv2d/bias:0' shape=(32,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'layer2/conv2d/kernel:0' shape=(3, 3, 32, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'layer2/conv2d/bias:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'layer1/conv2d/kernel:0' shape=(3, 3, 3, 32) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'layer1/conv2d/bias:0' shape=(32,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'layer2/conv2d/kernel:0' shape=(3, 3, 32, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'layer2/conv2d/bias:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'layer1/conv2d/kernel:0' shape=(3, 3, 3, 32) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'layer1/conv2d/bias:0' shape=(32,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'layer2/conv2d/kernel:0' shape=(3, 3, 32, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'layer2/conv2d/bias:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'layer1/conv2d/kernel:0' shape=(3, 3, 3, 32) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'layer1/conv2d/bias:0' shape=(32,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'layer2/conv2d/kernel:0' shape=(3, 3, 32, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'layer2/conv2d/bias:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "saved10-01-2020-Furhat__100101.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Catherine\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:228: RuntimeWarning: divide by zero encountered in int_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved10-01-2020-Furhat__9495.mp4\n",
      "saved10-01-2020-Furhat__9697.mp4\n",
      "saved10-01-2020-Furhat__9899.mp4\n",
      "saved10-02-2020-Furhat__102103.mp4\n",
      "saved10-02-2020-Furhat__104105.mp4\n",
      "saved10-02-2020-Furhat__106107 (1).mp4\n",
      "saved10-02-2020-Furhat__108109.mp4\n",
      "saved9-18-2020-Furhat__4041.mp4\n",
      "saved9-21-2020-Furhat__4243.mp4\n",
      "saved9-21-2020-Furhat__4445.mp4\n",
      "saved9-23-2020-Furhat__4647_NoReleaseRight.mp4\n",
      "saved9-23-2020-Furhat__4849.mp4\n",
      "saved9-23-2020-Furhat__5051.mp4\n",
      "saved9-23-2020-Furhat__5253.mp4\n",
      "saved9-23-2020-Furhat__5455.mp4\n",
      "saved9-23-2020-Furhat__5657.mp4\n",
      "saved9-24-2020-Furhat__5859.mp4\n",
      "saved9-24-2020-Furhat__6061_NoReleaseRight.mp4\n",
      "saved9-24-2020-Furhat__6263.mp4\n",
      "saved9-24-2020-Furhat__6465.mp4\n",
      "saved9-25-2020-Furhat__6667_NoReleaseRight.mp4\n",
      "saved9-25-2020-Furhat__6869.mp4\n",
      "saved9-25-2020-Furhat__7071.mp4\n",
      "saved9-25-2020-Furhat__7273.mp4\n",
      "saved9-26-2020-Furhat__7475.mp4\n",
      "saved9-28-2020-Furhat__7677_NoReleaseRight.mp4\n",
      "saved9-28-2020-Furhat__7879.mp4\n",
      "saved9-28-2020-Furhat__8081.mp4\n",
      "saved9-28-2020-Furhat__8283_NoReleaseRight.mp4\n",
      "saved9-29-2020-Furhat__8485.mp4\n",
      "saved9-29-2020-Furhat__8687.mp4\n",
      "saved9-29-2020-Furhat__9091.mp4\n",
      "saved9-30-2020-Furhat__9293.mp4\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jul 31 03:00:36 2020\n",
    "\n",
    "@author: hp\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "from Proctoring-AI.face_detector import get_face_detector, find_faces\n",
    "from Proctoring-AI.face_landmarks import get_landmark_model, detect_marks\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def get_2d_points(img, rotation_vector, translation_vector, camera_matrix, val):\n",
    "    \"\"\"Return the 3D points present as 2D for making annotation box\"\"\"\n",
    "    point_3d = []\n",
    "    dist_coeffs = np.zeros((4,1))\n",
    "    rear_size = val[0]\n",
    "    rear_depth = val[1]\n",
    "    point_3d.append((-rear_size, -rear_size, rear_depth))\n",
    "    point_3d.append((-rear_size, rear_size, rear_depth))\n",
    "    point_3d.append((rear_size, rear_size, rear_depth))\n",
    "    point_3d.append((rear_size, -rear_size, rear_depth))\n",
    "    point_3d.append((-rear_size, -rear_size, rear_depth))\n",
    "    \n",
    "    front_size = val[2]\n",
    "    front_depth = val[3]\n",
    "    point_3d.append((-front_size, -front_size, front_depth))\n",
    "    point_3d.append((-front_size, front_size, front_depth))\n",
    "    point_3d.append((front_size, front_size, front_depth))\n",
    "    point_3d.append((front_size, -front_size, front_depth))\n",
    "    point_3d.append((-front_size, -front_size, front_depth))\n",
    "    point_3d = np.array(point_3d, dtype=np.float).reshape(-1, 3)\n",
    "    \n",
    "    # Map to 2d img points\n",
    "    (point_2d, _) = cv2.projectPoints(point_3d,\n",
    "                                      rotation_vector,\n",
    "                                      translation_vector,\n",
    "                                      camera_matrix,\n",
    "                                      dist_coeffs)\n",
    "    point_2d = np.int32(point_2d.reshape(-1, 2))\n",
    "    return point_2d\n",
    "\n",
    "def draw_annotation_box(img, rotation_vector, translation_vector, camera_matrix,\n",
    "                        rear_size=300, rear_depth=0, front_size=500, front_depth=400,\n",
    "                        color=(255, 255, 0), line_width=2):\n",
    "    \"\"\"\n",
    "    Draw a 3D anotation box on the face for head pose estimation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.unit8\n",
    "        Original Image.\n",
    "    rotation_vector : Array of float64\n",
    "        Rotation Vector obtained from cv2.solvePnP\n",
    "    translation_vector : Array of float64\n",
    "        Translation Vector obtained from cv2.solvePnP\n",
    "    camera_matrix : Array of float64\n",
    "        The camera matrix\n",
    "    rear_size : int, optional\n",
    "        Size of rear box. The default is 300.\n",
    "    rear_depth : int, optional\n",
    "        The default is 0.\n",
    "    front_size : int, optional\n",
    "        Size of front box. The default is 500.\n",
    "    front_depth : int, optional\n",
    "        Front depth. The default is 400.\n",
    "    color : tuple, optional\n",
    "        The color with which to draw annotation box. The default is (255, 255, 0).\n",
    "    line_width : int, optional\n",
    "        line width of lines drawn. The default is 2.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    rear_size = 1\n",
    "    rear_depth = 0\n",
    "    front_size = img.shape[1]\n",
    "    front_depth = front_size*2\n",
    "    val = [rear_size, rear_depth, front_size, front_depth]\n",
    "    point_2d = get_2d_points(img, rotation_vector, translation_vector, camera_matrix, val)\n",
    "    # # Draw all the lines\n",
    "    cv2.polylines(img, [point_2d], True, color, line_width, cv2.LINE_AA)\n",
    "    cv2.line(img, tuple(point_2d[1]), tuple(\n",
    "        point_2d[6]), color, line_width, cv2.LINE_AA)\n",
    "    cv2.line(img, tuple(point_2d[2]), tuple(\n",
    "        point_2d[7]), color, line_width, cv2.LINE_AA)\n",
    "    cv2.line(img, tuple(point_2d[3]), tuple(\n",
    "        point_2d[8]), color, line_width, cv2.LINE_AA)\n",
    "    \n",
    "    \n",
    "def head_pose_points(img, rotation_vector, translation_vector, camera_matrix):\n",
    "    \"\"\"\n",
    "    Get the points to estimate head pose sideways    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.unit8\n",
    "        Original Image.\n",
    "    rotation_vector : Array of float64\n",
    "        Rotation Vector obtained from cv2.solvePnP\n",
    "    translation_vector : Array of float64\n",
    "        Translation Vector obtained from cv2.solvePnP\n",
    "    camera_matrix : Array of float64\n",
    "        The camera matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (x, y) : tuple\n",
    "        Coordinates of line to estimate head pose\n",
    "\n",
    "    \"\"\"\n",
    "    rear_size = 1\n",
    "    rear_depth = 0\n",
    "    front_size = img.shape[1]\n",
    "    front_depth = front_size*2\n",
    "    val = [rear_size, rear_depth, front_size, front_depth]\n",
    "    point_2d = get_2d_points(img, rotation_vector, translation_vector, camera_matrix, val)\n",
    "    y = (point_2d[5] + point_2d[8])//2\n",
    "    x = point_2d[2]\n",
    "    \n",
    "    return (x, y)\n",
    "\n",
    "import os\n",
    "\n",
    "directory = os.fsencode('C:/Users/Catherine/RPL_Language/collab-SLA/data_evaluation/Videos/')\n",
    "    \n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    video = 'C:/Users/Catherine/RPL_Language/collab-SLA/data_evaluation/Videos/'+filename\n",
    "    \n",
    "    face_model = get_face_detector()\n",
    "    landmark_model = get_landmark_model()\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    #cap = cv2.VideoCapture(0)\n",
    "    ret, img = cap.read()\n",
    "    size = img.shape\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "    # 3D model points.\n",
    "    model_points = np.array([\n",
    "                                (0.0, 0.0, 0.0),             # Nose tip\n",
    "                                (0.0, -330.0, -65.0),        # Chin\n",
    "                                (-225.0, 170.0, -135.0),     # Left eye left corner\n",
    "                                (225.0, 170.0, -135.0),      # Right eye right corne\n",
    "                                (-150.0, -150.0, -125.0),    # Left Mouth corner\n",
    "                                (150.0, -150.0, -125.0)      # Right mouth corner\n",
    "                            ])\n",
    "\n",
    "    # Camera internals\n",
    "    focal_length = size[1]\n",
    "    center = (size[1]/2, size[0]/2)\n",
    "    camera_matrix = np.array(\n",
    "                             [[focal_length, 0, center[0]],\n",
    "                             [0, focal_length, center[1]],\n",
    "                             [0, 0, 1]], dtype = \"double\"\n",
    "                             )\n",
    "\n",
    "    left = []\n",
    "    right = []\n",
    "    dateTimeObj = datetime.now()\n",
    "    start_time = dateTimeObj.second\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    totalNoFrames = cap.get(cv2.CAP_PROP_FRAME_COUNT);\n",
    "    durationInSeconds = float(totalNoFrames) / float(fps)\n",
    "    # cap.set(cv2.CAP_PROP_POS_AVI_RATIO,1)\n",
    "    # cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "    # print(cap.get(cv2.CAP_PROP_POS_MSEC))\n",
    "    # print(durationInSeconds)\\\n",
    "    left_pos = [0,0]\n",
    "    right_pos = [0,0]\n",
    "    old_time = cap.get(cv2.CAP_PROP_POS_MSEC)/1000\n",
    "    while True:\n",
    "        ret, img = cap.read()\n",
    "        if ret == True:\n",
    "            new_time = cap.get(cv2.CAP_PROP_POS_MSEC)/1000\n",
    "            if new_time - old_time >= 0.1:\n",
    "                faces = find_faces(img, face_model)\n",
    "                for face in faces:\n",
    "                    if face[0] > 1000:\n",
    "                        person = 'left'\n",
    "                    else:\n",
    "                        person = 'right'\n",
    "                    marks = detect_marks(img, landmark_model, face)\n",
    "                    # mark_detector.draw_marks(img, marks, color=(0, 255, 0))\n",
    "                    image_points = np.array([\n",
    "                                            marks[30],     # Nose tip\n",
    "                                            marks[8],     # Chin\n",
    "                                            marks[36],     # Left eye left corner\n",
    "                                            marks[45],     # Right eye right corne\n",
    "                                            marks[48],     # Left Mouth corner\n",
    "                                            marks[54]      # Right mouth corner\n",
    "                                        ], dtype=\"double\")\n",
    "                    dist_coeffs = np.zeros((4,1)) # Assuming no lens distortion\n",
    "                    (success, rotation_vector, translation_vector) = cv2.solvePnP(model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_UPNP)\n",
    "\n",
    "\n",
    "                    # Project a 3D point (0, 0, 1000.0) onto the image plane.\n",
    "                    # We use this to draw a line sticking out of the nose\n",
    "\n",
    "                    (nose_end_point2D, jacobian) = cv2.projectPoints(np.array([(0.0, 0.0, 1000.0)]), rotation_vector, translation_vector, camera_matrix, dist_coeffs)\n",
    "\n",
    "                    for p in image_points:\n",
    "                        cv2.circle(img, (int(p[0]), int(p[1])), 3, (0,0,255), -1)\n",
    "\n",
    "\n",
    "                    p1 = ( int(image_points[0][0]), int(image_points[0][1]))\n",
    "                    p2 = ( int(nose_end_point2D[0][0][0]), int(nose_end_point2D[0][0][1]))\n",
    "                    x1, x2 = head_pose_points(img, rotation_vector, translation_vector, camera_matrix)\n",
    "\n",
    "                    cv2.line(img, p1, p2, (0, 255, 255), 2)\n",
    "                    cv2.line(img, tuple(x1), tuple(x2), (255, 255, 0), 2)\n",
    "                    # for (x, y) in marks:\n",
    "                    #     cv2.circle(img, (x, y), 4, (255, 255, 0), -1)\n",
    "                    # cv2.putText(img, str(p1), p1, font, 1, (0, 255, 255), 1)\n",
    "                    try:\n",
    "                        m = (p2[1] - p1[1])/(p2[0] - p1[0])\n",
    "                        ang1 = int(math.degrees(math.atan(m)))\n",
    "                    except:\n",
    "                        ang1 = 90\n",
    "\n",
    "                    try:\n",
    "                        m = (x2[1] - x1[1])/(x2[0] - x1[0])\n",
    "                        ang2 = int(math.degrees(math.atan(-1/m)))\n",
    "                    except:\n",
    "                        ang2 = 90\n",
    "\n",
    "                        # print('div by zero error')\n",
    "                    if ang1 >= 48:\n",
    "                        #print(person,face,'Head down')\n",
    "                        cv2.putText(img, 'Head down', (30, 30), font, 2, (255, 255, 128), 3)\n",
    "                    elif ang1 <= -48:\n",
    "                        #print(person,face,'Head up')\n",
    "                        cv2.putText(img, 'Head up', (30, 30), font, 2, (255, 255, 128), 3)\n",
    "\n",
    "                    if ang2 >= 48:\n",
    "                        #print(person,face,'Head right')\n",
    "                        cv2.putText(img, 'Head right', (90, 30), font, 2, (255, 255, 128), 3)\n",
    "                    elif ang2 <= -48:\n",
    "                        #print(person,face,'Head left')\n",
    "                        cv2.putText(img, 'Head left', (90, 30), font, 2, (255, 255, 128), 3)\n",
    "                    if person == 'left':\n",
    "                        left_pos = [ang1,ang2]\n",
    "                    if person == 'right':\n",
    "                        right_pos = [ang1,ang2]\n",
    "                    cv2.putText(img, str(ang1), tuple(p1), font, 2, (128, 255, 255), 3)\n",
    "                    cv2.putText(img, str(ang2), tuple(x1), font, 2, (255, 255, 128), 3)\n",
    "                left.append([new_time,left_pos[0],left_pos[1]])\n",
    "                right.append([new_time,right_pos[0],right_pos[1]])\n",
    "                old_time = new_time\n",
    "                cv2.imshow('img', img)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "        else:\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "    leftdf = pd.DataFrame(left)\n",
    "    leftdf.columns = ['time_elapsed (s)', 'left angle vertical', 'left angle horizontal']\n",
    "    rightdf = pd.DataFrame(right)\n",
    "    rightdf.columns = ['time_elapsed (s)', 'right angle vertical', 'right angle horizontal']\n",
    "    combined_results = leftdf\n",
    "    combined_results['right angle vertical'] = rightdf['right angle vertical']\n",
    "    combined_results['right angle horizontal'] = rightdf['right angle horizontal']\n",
    "    combined_results.to_csv('participant_csvs/'+filename[:-4]+'.csv')\n",
    "    print('saved'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
